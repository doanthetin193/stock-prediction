"""
Module Sentiment Analysis cho tin tá»©c tÃ i chÃ­nh Viá»‡t Nam.
Crawl tin tá»©c vÃ  phÃ¢n tÃ­ch sentiment score.
(ÄÃ¢y lÃ  module bonus - hoáº¡t Ä‘á»™ng Ä‘á»™c láº­p)
"""
import os
import re
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

import sys
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import DATA_DIR


# ============================================================
# Crawl Tin Tá»©c
# ============================================================

def crawl_cafef_news(symbol: str, max_pages: int = 3) -> list:
    """
    Crawl tin tá»©c tá»« CafeF liÃªn quan Ä‘áº¿n mÃ£ cá»• phiáº¿u.

    Args:
        symbol: MÃ£ cá»• phiáº¿u (VNM, FPT, ...)
        max_pages: Sá»‘ trang crawl

    Returns:
        List of dict: [{'title': ..., 'date': ..., 'content': ...}]
    """
    articles = []
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

    for page in range(1, max_pages + 1):
        try:
            url = f"https://cafef.vn/tim-kiem.chn?keywords={symbol}&page={page}"
            response = requests.get(url, headers=headers, timeout=10)
            response.encoding = 'utf-8'

            soup = BeautifulSoup(response.text, 'html.parser')

            # TÃ¬m cÃ¡c bÃ i viáº¿t
            items = soup.find_all('div', class_='tlitem') or soup.find_all('li', class_='news-item')

            for item in items:
                try:
                    title_tag = item.find('a', class_='title') or item.find('h3')
                    if title_tag:
                        title = title_tag.get_text(strip=True)

                        # Láº¥y ngÃ y
                        date_tag = item.find('span', class_='time') or item.find('time')
                        date_str = date_tag.get_text(strip=True) if date_tag else ""

                        articles.append({
                            'title': title,
                            'date': date_str,
                            'symbol': symbol
                        })
                except Exception:
                    continue

            print(f"  ğŸ“° CafeF page {page}: {len(items)} bÃ i viáº¿t")

        except Exception as e:
            print(f"  âš ï¸ Lá»—i crawl CafeF page {page}: {e}")

    return articles


def crawl_vnexpress_news(symbol: str, max_pages: int = 3) -> list:
    """
    Crawl tin tá»©c tá»« VnExpress pháº§n kinh doanh/chá»©ng khoÃ¡n.

    Args:
        symbol: MÃ£ cá»• phiáº¿u
        max_pages: Sá»‘ trang crawl

    Returns:
        List of dict
    """
    articles = []
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

    for page in range(1, max_pages + 1):
        try:
            url = f"https://timkiem.vnexpress.net/?q={symbol}&cate_code=kinhdoanh&page={page}"
            response = requests.get(url, headers=headers, timeout=10)
            response.encoding = 'utf-8'

            soup = BeautifulSoup(response.text, 'html.parser')
            items = soup.find_all('article', class_='item-news')

            for item in items:
                try:
                    title_tag = item.find('h3') or item.find('h2')
                    if title_tag:
                        a_tag = title_tag.find('a')
                        title = a_tag.get_text(strip=True) if a_tag else title_tag.get_text(strip=True)

                        desc_tag = item.find('p', class_='description')
                        desc = desc_tag.get_text(strip=True) if desc_tag else ""

                        date_tag = item.find('span', class_='time-ago')
                        date_str = date_tag.get_text(strip=True) if date_tag else ""

                        articles.append({
                            'title': title,
                            'description': desc,
                            'date': date_str,
                            'symbol': symbol
                        })
                except Exception:
                    continue

            print(f"  ğŸ“° VnExpress page {page}: {len(items)} bÃ i viáº¿t")

        except Exception as e:
            print(f"  âš ï¸ Lá»—i crawl VnExpress page {page}: {e}")

    return articles


# ============================================================
# Sentiment Analysis
# ============================================================

def analyze_sentiment_textblob(text: str) -> float:
    """
    PhÃ¢n tÃ­ch sentiment báº±ng TextBlob.
    ÄÆ¡n giáº£n, hoáº¡t Ä‘á»™ng tá»‘t vá»›i tiáº¿ng Anh, OK cho tiáº¿ng Viá»‡t.

    Returns:
        Score tá»« -1 (tiÃªu cá»±c) Ä‘áº¿n +1 (tÃ­ch cá»±c)
    """
    try:
        from textblob import TextBlob
        blob = TextBlob(text)
        return blob.sentiment.polarity
    except Exception:
        return 0.0


def analyze_sentiment_vietnamese(text: str) -> float:
    """
    PhÃ¢n tÃ­ch sentiment cho tiáº¿ng Viá»‡t báº±ng tá»« Ä‘iá»ƒn.
    ÄÆ¡n giáº£n nhÆ°ng hiá»‡u quáº£ cho tin tá»©c tÃ i chÃ­nh.

    Returns:
        Score tá»« -1 Ä‘áº¿n +1
    """
    # Tá»« Ä‘iá»ƒn sentiment cÆ¡ báº£n cho tÃ i chÃ­nh Viá»‡t Nam
    positive_words = [
        'tÄƒng', 'lÃ£i', 'tÃ­ch cá»±c', 'tá»‘t', 'tÄƒng trÆ°á»Ÿng', 'ká»· lá»¥c',
        'Ä‘á»™t phÃ¡', 'vÆ°á»£t', 'thuáº­n lá»£i', 'kháº£ quan', 'bá»©t phÃ¡',
        'há»“i phá»¥c', 'triá»ƒn vá»ng', 'láº¡c quan', 'Ä‘Ã  tÄƒng', 'Ä‘iá»ƒm sÃ¡ng',
        'cáº£i thiá»‡n', 'hiá»‡u quáº£', 'tháº·ng dÆ°', 'dáº«n Ä‘áº§u', 'bá»n vá»¯ng',
        'phÃ¡t triá»ƒn', 'má»Ÿ rá»™ng', 'doanh thu tÄƒng', 'lá»£i nhuáº­n tÄƒng'
    ]

    negative_words = [
        'giáº£m', 'lá»—', 'tiÃªu cá»±c', 'xáº¥u', 'sá»¥t giáº£m', 'rá»§i ro',
        'khÃ³ khÄƒn', 'tá»¥t', 'báº¥t lá»£i', 'lo ngáº¡i', 'suy yáº¿u',
        'Ä‘Ã¡y', 'Ä‘á»• vá»¡', 'bi quan', 'Ä‘Ã  giáº£m', 'cáº£nh bÃ¡o',
        'ná»£ xáº¥u', 'thua lá»—', 'phÃ¡ sáº£n', 'khá»§ng hoáº£ng', 'sá»¥p Ä‘á»•',
        'bÃ¡n thÃ¡o', 'lao dá»‘c', 'thÃ¢m há»¥t', 'doanh thu giáº£m'
    ]

    text_lower = text.lower()
    pos_count = sum(1 for word in positive_words if word in text_lower)
    neg_count = sum(1 for word in negative_words if word in text_lower)

    total = pos_count + neg_count
    if total == 0:
        return 0.0

    score = (pos_count - neg_count) / total
    return round(score, 4)


def get_sentiment_for_stock(symbol: str, use_vietnamese: bool = True) -> pd.DataFrame:
    """
    Crawl tin tá»©c vÃ  tÃ­nh sentiment score cho mÃ£ cá»• phiáº¿u.

    Args:
        symbol: MÃ£ cá»• phiáº¿u
        use_vietnamese: True = dÃ¹ng tá»« Ä‘iá»ƒn tiáº¿ng Viá»‡t, False = dÃ¹ng TextBlob

    Returns:
        DataFrame (title, date, sentiment_score)
    """
    print(f"\nğŸ” Crawl & phÃ¢n tÃ­ch sentiment cho {symbol}...")

    # Crawl tin tá»©c
    articles = crawl_cafef_news(symbol)
    articles.extend(crawl_vnexpress_news(symbol))

    if not articles:
        print(f"  âš ï¸ KhÃ´ng tÃ¬m Ä‘Æ°á»£c tin tá»©c cho {symbol}")
        return pd.DataFrame(columns=['title', 'date', 'sentiment_score'])

    # PhÃ¢n tÃ­ch sentiment
    results = []
    for article in articles:
        text = article.get('title', '') + ' ' + article.get('description', '')

        if use_vietnamese:
            score = analyze_sentiment_vietnamese(text)
        else:
            score = analyze_sentiment_textblob(text)

        results.append({
            'title': article.get('title', ''),
            'date': article.get('date', ''),
            'sentiment_score': score
        })

    df = pd.DataFrame(results)

    # Thá»‘ng kÃª
    avg_score = df['sentiment_score'].mean()
    sentiment_label = "TÃ­ch cá»±c ğŸ“ˆ" if avg_score > 0 else "TiÃªu cá»±c ğŸ“‰" if avg_score < 0 else "Trung láº­p â¡ï¸"

    print(f"  ğŸ“Š Tá»•ng: {len(df)} tin tá»©c")
    print(f"  ğŸ“Š Sentiment trung bÃ¬nh: {avg_score:.4f} ({sentiment_label})")

    return df


def save_sentiment_data(df: pd.DataFrame, symbol: str) -> str:
    """LÆ°u sentiment data ra CSV."""
    os.makedirs(DATA_DIR, exist_ok=True)
    filepath = os.path.join(DATA_DIR, f"{symbol}_sentiment.csv")
    df.to_csv(filepath, index=False, encoding='utf-8-sig')
    print(f"  ğŸ’¾ ÄÃ£ lÆ°u sentiment: {filepath}")
    return filepath


if __name__ == "__main__":
    # Demo: crawl sentiment cho VNM
    df = get_sentiment_for_stock("VNM")
    if not df.empty:
        save_sentiment_data(df, "VNM")
        print(df.head(10))
