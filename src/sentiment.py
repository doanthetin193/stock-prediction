"""
Module Sentiment Analysis cho tin t·ª©c t√†i ch√≠nh Vi·ªát Nam.
- Market-based sentiment: t√≠nh t·ª´ d·ªØ li·ªáu gi√° (lu√¥n ho·∫°t ƒë·ªông)
- News-based sentiment: crawl tin t·ª©c t·ª´ CafeF/VnExpress (c·∫ßn bs4, requests)
"""
import os
import re
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

import sys
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import DATA_DIR


# ============================================================
# Crawl Tin T·ª©c
# ============================================================

def crawl_cafef_news(symbol: str, max_pages: int = 3) -> list:
    """
    Crawl tin t·ª©c t·ª´ CafeF li√™n quan ƒë·∫øn m√£ c·ªï phi·∫øu.

    Args:
        symbol: M√£ c·ªï phi·∫øu (VNM, FPT, ...)
        max_pages: S·ªë trang crawl

    Returns:
        List of dict: [{'title': ..., 'date': ..., 'content': ...}]
    """
    articles = []
    import requests
    from bs4 import BeautifulSoup
    import time
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

    for page in range(1, max_pages + 1):
        try:
            url = f"https://cafef.vn/tim-kiem.chn?keywords={symbol}&page={page}"
            response = requests.get(url, headers=headers, timeout=10)
            response.encoding = 'utf-8'

            soup = BeautifulSoup(response.text, 'html.parser')

            # T√¨m c√°c b√†i vi·∫øt
            items = soup.find_all('div', class_='tlitem') or soup.find_all('li', class_='news-item')

            for item in items:
                try:
                    title_tag = item.find('a', class_='title') or item.find('h3')
                    if title_tag:
                        title = title_tag.get_text(strip=True)

                        # L·∫•y ng√†y
                        date_tag = item.find('span', class_='time') or item.find('time')
                        date_str = date_tag.get_text(strip=True) if date_tag else ""

                        articles.append({
                            'title': title,
                            'date': date_str,
                            'symbol': symbol
                        })
                except Exception:
                    continue

            print(f"  üì∞ CafeF page {page}: {len(items)} b√†i vi·∫øt")
            if page < max_pages:
                time.sleep(1)  # Tr√°nh b·ªã block

        except Exception as e:
            print(f"  ‚ö†Ô∏è L·ªói crawl CafeF page {page}: {e}")

    return articles


def crawl_vnexpress_news(symbol: str, max_pages: int = 3) -> list:
    """
    Crawl tin t·ª©c t·ª´ VnExpress ph·∫ßn kinh doanh/ch·ª©ng kho√°n.

    Args:
        symbol: M√£ c·ªï phi·∫øu
        max_pages: S·ªë trang crawl

    Returns:
        List of dict
    """
    articles = []
    import requests
    from bs4 import BeautifulSoup
    import time
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

    for page in range(1, max_pages + 1):
        try:
            url = f"https://timkiem.vnexpress.net/?q={symbol}&cate_code=kinhdoanh&page={page}"
            response = requests.get(url, headers=headers, timeout=10)
            response.encoding = 'utf-8'

            soup = BeautifulSoup(response.text, 'html.parser')
            items = soup.find_all('article', class_='item-news')

            for item in items:
                try:
                    title_tag = item.find('h3') or item.find('h2')
                    if title_tag:
                        a_tag = title_tag.find('a')
                        title = a_tag.get_text(strip=True) if a_tag else title_tag.get_text(strip=True)

                        desc_tag = item.find('p', class_='description')
                        desc = desc_tag.get_text(strip=True) if desc_tag else ""

                        date_tag = item.find('span', class_='time-ago')
                        date_str = date_tag.get_text(strip=True) if date_tag else ""

                        articles.append({
                            'title': title,
                            'description': desc,
                            'date': date_str,
                            'symbol': symbol
                        })
                except Exception:
                    continue

            print(f"  üì∞ VnExpress page {page}: {len(items)} b√†i vi·∫øt")
            if page < max_pages:
                time.sleep(1)  # Tr√°nh b·ªã block

        except Exception as e:
            print(f"  ‚ö†Ô∏è L·ªói crawl VnExpress page {page}: {e}")

    return articles


# ============================================================
# Sentiment Analysis
# ============================================================

def analyze_sentiment_textblob(text: str) -> float:
    """
    Ph√¢n t√≠ch sentiment b·∫±ng TextBlob.
    ƒê∆°n gi·∫£n, ho·∫°t ƒë·ªông t·ªët v·ªõi ti·∫øng Anh, OK cho ti·∫øng Vi·ªát.

    Returns:
        Score t·ª´ -1 (ti√™u c·ª±c) ƒë·∫øn +1 (t√≠ch c·ª±c)
    """
    try:
        from textblob import TextBlob
        blob = TextBlob(text)
        return blob.sentiment.polarity
    except Exception:
        return 0.0


def analyze_sentiment_vietnamese(text: str) -> float:
    """
    Ph√¢n t√≠ch sentiment cho ti·∫øng Vi·ªát b·∫±ng t·ª´ ƒëi·ªÉn.
    ƒê∆°n gi·∫£n nh∆∞ng hi·ªáu qu·∫£ cho tin t·ª©c t√†i ch√≠nh.

    Returns:
        Score t·ª´ -1 ƒë·∫øn +1
    """
    # T·ª´ ƒëi·ªÉn sentiment c∆° b·∫£n cho t√†i ch√≠nh Vi·ªát Nam
    positive_words = [
        'tƒÉng', 'l√£i', 't√≠ch c·ª±c', 't·ªët', 'tƒÉng tr∆∞·ªüng', 'k·ª∑ l·ª•c',
        'ƒë·ªôt ph√°', 'v∆∞·ª£t', 'thu·∫≠n l·ª£i', 'kh·∫£ quan', 'b·ª©t ph√°',
        'h·ªìi ph·ª•c', 'tri·ªÉn v·ªçng', 'l·∫°c quan', 'ƒë√† tƒÉng', 'ƒëi·ªÉm s√°ng',
        'c·∫£i thi·ªán', 'hi·ªáu qu·∫£', 'th·∫∑ng d∆∞', 'd·∫´n ƒë·∫ßu', 'b·ªÅn v·ªØng',
        'ph√°t tri·ªÉn', 'm·ªü r·ªông', 'doanh thu tƒÉng', 'l·ª£i nhu·∫≠n tƒÉng'
    ]

    negative_words = [
        'gi·∫£m', 'l·ªó', 'ti√™u c·ª±c', 'x·∫•u', 's·ª•t gi·∫£m', 'r·ªßi ro',
        'kh√≥ khƒÉn', 't·ª•t', 'b·∫•t l·ª£i', 'lo ng·∫°i', 'suy y·∫øu',
        'ƒë√°y', 'ƒë·ªï v·ª°', 'bi quan', 'ƒë√† gi·∫£m', 'c·∫£nh b√°o',
        'n·ª£ x·∫•u', 'thua l·ªó', 'ph√° s·∫£n', 'kh·ªßng ho·∫£ng', 's·ª•p ƒë·ªï',
        'b√°n th√°o', 'lao d·ªëc', 'th√¢m h·ª•t', 'doanh thu gi·∫£m'
    ]

    text_lower = text.lower()
    pos_count = sum(1 for word in positive_words if word in text_lower)
    neg_count = sum(1 for word in negative_words if word in text_lower)

    total = pos_count + neg_count
    if total == 0:
        return 0.0

    score = (pos_count - neg_count) / total
    return round(score, 4)


def get_sentiment_for_stock(symbol: str, use_vietnamese: bool = True) -> pd.DataFrame:
    """
    Crawl tin t·ª©c v√† t√≠nh sentiment score cho m√£ c·ªï phi·∫øu.

    Args:
        symbol: M√£ c·ªï phi·∫øu
        use_vietnamese: True = d√πng t·ª´ ƒëi·ªÉn ti·∫øng Vi·ªát, False = d√πng TextBlob

    Returns:
        DataFrame (title, date, sentiment_score)
    """
    print(f"\nüîç Crawl & ph√¢n t√≠ch sentiment cho {symbol}...")

    # Crawl tin t·ª©c
    articles = crawl_cafef_news(symbol)
    articles.extend(crawl_vnexpress_news(symbol))

    if not articles:
        print(f"  ‚ö†Ô∏è Kh√¥ng t√¨m ƒë∆∞·ª£c tin t·ª©c cho {symbol}")
        return pd.DataFrame(columns=['title', 'date', 'sentiment_score'])

    # Ph√¢n t√≠ch sentiment
    results = []
    for article in articles:
        text = article.get('title', '') + ' ' + article.get('description', '')

        if use_vietnamese:
            score = analyze_sentiment_vietnamese(text)
        else:
            score = analyze_sentiment_textblob(text)

        results.append({
            'title': article.get('title', ''),
            'date': article.get('date', ''),
            'sentiment_score': score
        })

    df = pd.DataFrame(results)

    # Th·ªëng k√™
    avg_score = df['sentiment_score'].mean()
    sentiment_label = "T√≠ch c·ª±c üìà" if avg_score > 0 else "Ti√™u c·ª±c üìâ" if avg_score < 0 else "Trung l·∫≠p ‚û°Ô∏è"

    print(f"  üìä T·ªïng: {len(df)} tin t·ª©c")
    print(f"  üìä Sentiment trung b√¨nh: {avg_score:.4f} ({sentiment_label})")

    return df


def save_sentiment_data(df: pd.DataFrame, symbol: str) -> str:
    """L∆∞u sentiment data ra CSV."""
    os.makedirs(DATA_DIR, exist_ok=True)
    filepath = os.path.join(DATA_DIR, f"{symbol}_sentiment.csv")
    df.to_csv(filepath, index=False, encoding='utf-8-sig')
    print(f"  üíæ ƒê√£ l∆∞u sentiment: {filepath}")
    return filepath


# ============================================================
# Market-based Sentiment (t√≠nh t·ª´ d·ªØ li·ªáu gi√°)
# ============================================================

def compute_market_sentiment(df: pd.DataFrame) -> pd.DataFrame:
    """
    T√≠nh market-based sentiment t·ª´ d·ªØ li·ªáu gi√° c·ªï phi·∫øu.
    Wam d·ª•ng √Ω t∆∞·ªüng: gi√° ph·∫£n √°nh t√¢m l√Ω th·ªã tr∆∞·ªùng.

    T·∫°o 3 features:
    - sentiment_momentum: d·ª±a tr√™n xu h∆∞·ªõng gi√° 5 ng√†y g·∫ßn nh·∫•t
    - sentiment_volatility: d·ª±a tr√™n bi·∫øn ƒë·ªông gi√° (cao = b·∫•t ·ªïn = ti√™u c·ª±c)
    - sentiment_score: t·ªïng h·ª£p = momentum * 0.6 + volume_signal * 0.2 - volatility * 0.2

    Returns:
        DataFrame g·ªëc + 3 c·ªôt sentiment m·ªõi
    """
    df = df.copy()

    # --- 1. Momentum Sentiment (xu h∆∞·ªõng gi√° 5 ng√†y) ---
    # Return 5 ng√†y: > 0 = t√≠ch c·ª±c, < 0 = ti√™u c·ª±c
    returns_5d = df['close'].pct_change(5)
    # Chu·∫©n h√≥a v·ªÅ [-1, 1] b·∫±ng tanh (sigmoid-like, smooth)
    df['sentiment_momentum'] = np.tanh(returns_5d * 10)

    # --- 2. Volatility Sentiment (bi·∫øn ƒë·ªông = b·∫•t ·ªïn) ---
    # Std of returns trong 10 ng√†y, chu·∫©n h√≥a
    daily_returns = df['close'].pct_change()
    rolling_vol = daily_returns.rolling(10).std()
    # Chu·∫©n h√≥a: vol cao ‚Üí sentiment th·∫•p (b·∫•t ·ªïn = ti√™u c·ª±c)
    vol_median = rolling_vol.median()
    if vol_median > 0:
        vol_normalized = rolling_vol / (vol_median * 3)  # scale
        df['sentiment_volatility'] = vol_normalized.clip(0, 1)
    else:
        df['sentiment_volatility'] = 0.0

    # --- 3. Volume Signal ---
    # Volume tƒÉng ƒë·ªôt bi·∫øn khi c√≥ tin t·ª©c/s·ª± ki·ªán
    vol_sma20 = df['volume'].rolling(20).mean()
    volume_ratio = df['volume'] / vol_sma20
    # Volume > 1.5x trung b√¨nh = s·ª± ki·ªán, nh√¢n v·ªõi h∆∞·ªõng gi√°
    volume_signal = np.tanh((volume_ratio - 1) * 2) * np.sign(daily_returns)

    # --- 4. T·ªïng h·ª£p Sentiment Score ---
    df['sentiment_score'] = (
        df['sentiment_momentum'] * 0.6 +
        volume_signal.fillna(0) * 0.2 -
        df['sentiment_volatility'] * 0.2
    ).round(4)

    # Clip v·ªÅ [-1, 1]
    df['sentiment_score'] = df['sentiment_score'].clip(-1, 1)

    # Fill NaN (t·ª´ rolling) b·∫±ng 0 (neutral)
    for col in ['sentiment_momentum', 'sentiment_volatility', 'sentiment_score']:
        df[col] = df[col].fillna(0)

    print(f"  üìä Market Sentiment: mean={df['sentiment_score'].mean():.4f}, "
          f"positive={( df['sentiment_score'] > 0).sum()}, "
          f"negative={(df['sentiment_score'] < 0).sum()}")

    return df


def merge_sentiment_with_data(df: pd.DataFrame, symbol: str) -> pd.DataFrame:
    """
    Merge sentiment v√†o DataFrame c·ªï phi·∫øu.
    - Market-based sentiment: lu√¥n c√≥ (t√≠nh t·ª´ gi√°)
    - News-based sentiment: override n·∫øu c√≥ file CSV ƒë√£ crawl

    Args:
        df: DataFrame c·ªï phi·∫øu g·ªëc (c·∫ßn c·ªôt 'time', 'close', 'volume')
        symbol: m√£ c·ªï phi·∫øu

    Returns:
        DataFrame + 3 c·ªôt: sentiment_score, sentiment_momentum, sentiment_volatility
    """
    # B∆∞·ªõc 1: T√≠nh market-based sentiment cho to√†n b·ªô l·ªãch s·ª≠
    df = compute_market_sentiment(df)

    # B∆∞·ªõc 2: Override b·∫±ng news-based n·∫øu c√≥
    news_file = os.path.join(DATA_DIR, f"{symbol}_sentiment.csv")
    if os.path.exists(news_file):
        try:
            news_df = pd.read_csv(news_file)
            if 'date' in news_df.columns and 'sentiment_score' in news_df.columns:
                # Parse ng√†y t·ª´ news data
                news_df['date'] = pd.to_datetime(news_df['date'], errors='coerce')
                news_df = news_df.dropna(subset=['date'])

                if not news_df.empty:
                    # Aggregate: trung b√¨nh sentiment theo ng√†y
                    daily_news = news_df.groupby(news_df['date'].dt.date)['sentiment_score'].mean()

                    # Override market-based b·∫±ng news-based cho nh·ªØng ng√†y c√≥ tin
                    df_dates = pd.to_datetime(df['time']).dt.date
                    for date_val, score in daily_news.items():
                        mask = df_dates == date_val
                        if mask.any():
                            # K·∫øt h·ª£p: 50% market + 50% news
                            market_score = df.loc[mask, 'sentiment_score'].values[0]
                            blended = market_score * 0.5 + score * 0.5
                            df.loc[mask, 'sentiment_score'] = round(blended, 4)

                    n_overridden = sum(1 for d in daily_news.index if (df_dates == d).any())
                    print(f"  üì∞ News sentiment merged: {n_overridden} ng√†y ƒë∆∞·ª£c c·∫≠p nh·∫≠t t·ª´ tin t·ª©c")

        except Exception as e:
            print(f"  ‚ö†Ô∏è Kh√¥ng th·ªÉ merge news sentiment: {e}")

    return df


if __name__ == "__main__":
    # Demo: crawl sentiment cho VNM
    df = get_sentiment_for_stock("VNM")
    if not df.empty:
        save_sentiment_data(df, "VNM")
        print(df.head(10))

