"""
Module Sentiment Analysis cho tin t·ª©c t√†i ch√≠nh Vi·ªát Nam.
Crawl tin t·ª©c v√† ph√¢n t√≠ch sentiment score.
(ƒê√¢y l√† module bonus - ho·∫°t ƒë·ªông ƒë·ªôc l·∫≠p)
"""
import os
import re
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time
import warnings
warnings.filterwarnings('ignore')

import sys
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import DATA_DIR


# ============================================================
# Crawl Tin T·ª©c
# ============================================================

def crawl_cafef_news(symbol: str, max_pages: int = 3) -> list:
    """
    Crawl tin t·ª©c t·ª´ CafeF li√™n quan ƒë·∫øn m√£ c·ªï phi·∫øu.

    Args:
        symbol: M√£ c·ªï phi·∫øu (VNM, FPT, ...)
        max_pages: S·ªë trang crawl

    Returns:
        List of dict: [{'title': ..., 'date': ..., 'content': ...}]
    """
    articles = []
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

    for page in range(1, max_pages + 1):
        try:
            url = f"https://cafef.vn/tim-kiem.chn?keywords={symbol}&page={page}"
            response = requests.get(url, headers=headers, timeout=10)
            response.encoding = 'utf-8'

            soup = BeautifulSoup(response.text, 'html.parser')

            # T√¨m c√°c b√†i vi·∫øt
            items = soup.find_all('div', class_='tlitem') or soup.find_all('li', class_='news-item')

            for item in items:
                try:
                    title_tag = item.find('a', class_='title') or item.find('h3')
                    if title_tag:
                        title = title_tag.get_text(strip=True)

                        # L·∫•y ng√†y
                        date_tag = item.find('span', class_='time') or item.find('time')
                        date_str = date_tag.get_text(strip=True) if date_tag else ""

                        articles.append({
                            'title': title,
                            'date': date_str,
                            'symbol': symbol
                        })
                except Exception:
                    continue

            print(f"  üì∞ CafeF page {page}: {len(items)} b√†i vi·∫øt")
            if page < max_pages:
                time.sleep(1)  # Tr√°nh b·ªã block

        except Exception as e:
            print(f"  ‚ö†Ô∏è L·ªói crawl CafeF page {page}: {e}")

    return articles


def crawl_vnexpress_news(symbol: str, max_pages: int = 3) -> list:
    """
    Crawl tin t·ª©c t·ª´ VnExpress ph·∫ßn kinh doanh/ch·ª©ng kho√°n.

    Args:
        symbol: M√£ c·ªï phi·∫øu
        max_pages: S·ªë trang crawl

    Returns:
        List of dict
    """
    articles = []
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

    for page in range(1, max_pages + 1):
        try:
            url = f"https://timkiem.vnexpress.net/?q={symbol}&cate_code=kinhdoanh&page={page}"
            response = requests.get(url, headers=headers, timeout=10)
            response.encoding = 'utf-8'

            soup = BeautifulSoup(response.text, 'html.parser')
            items = soup.find_all('article', class_='item-news')

            for item in items:
                try:
                    title_tag = item.find('h3') or item.find('h2')
                    if title_tag:
                        a_tag = title_tag.find('a')
                        title = a_tag.get_text(strip=True) if a_tag else title_tag.get_text(strip=True)

                        desc_tag = item.find('p', class_='description')
                        desc = desc_tag.get_text(strip=True) if desc_tag else ""

                        date_tag = item.find('span', class_='time-ago')
                        date_str = date_tag.get_text(strip=True) if date_tag else ""

                        articles.append({
                            'title': title,
                            'description': desc,
                            'date': date_str,
                            'symbol': symbol
                        })
                except Exception:
                    continue

            print(f"  üì∞ VnExpress page {page}: {len(items)} b√†i vi·∫øt")
            if page < max_pages:
                time.sleep(1)  # Tr√°nh b·ªã block

        except Exception as e:
            print(f"  ‚ö†Ô∏è L·ªói crawl VnExpress page {page}: {e}")

    return articles


# ============================================================
# Sentiment Analysis
# ============================================================

def analyze_sentiment_textblob(text: str) -> float:
    """
    Ph√¢n t√≠ch sentiment b·∫±ng TextBlob.
    ƒê∆°n gi·∫£n, ho·∫°t ƒë·ªông t·ªët v·ªõi ti·∫øng Anh, OK cho ti·∫øng Vi·ªát.

    Returns:
        Score t·ª´ -1 (ti√™u c·ª±c) ƒë·∫øn +1 (t√≠ch c·ª±c)
    """
    try:
        from textblob import TextBlob
        blob = TextBlob(text)
        return blob.sentiment.polarity
    except Exception:
        return 0.0


def analyze_sentiment_vietnamese(text: str) -> float:
    """
    Ph√¢n t√≠ch sentiment cho ti·∫øng Vi·ªát b·∫±ng t·ª´ ƒëi·ªÉn.
    ƒê∆°n gi·∫£n nh∆∞ng hi·ªáu qu·∫£ cho tin t·ª©c t√†i ch√≠nh.

    Returns:
        Score t·ª´ -1 ƒë·∫øn +1
    """
    # T·ª´ ƒëi·ªÉn sentiment c∆° b·∫£n cho t√†i ch√≠nh Vi·ªát Nam
    positive_words = [
        'tƒÉng', 'l√£i', 't√≠ch c·ª±c', 't·ªët', 'tƒÉng tr∆∞·ªüng', 'k·ª∑ l·ª•c',
        'ƒë·ªôt ph√°', 'v∆∞·ª£t', 'thu·∫≠n l·ª£i', 'kh·∫£ quan', 'b·ª©t ph√°',
        'h·ªìi ph·ª•c', 'tri·ªÉn v·ªçng', 'l·∫°c quan', 'ƒë√† tƒÉng', 'ƒëi·ªÉm s√°ng',
        'c·∫£i thi·ªán', 'hi·ªáu qu·∫£', 'th·∫∑ng d∆∞', 'd·∫´n ƒë·∫ßu', 'b·ªÅn v·ªØng',
        'ph√°t tri·ªÉn', 'm·ªü r·ªông', 'doanh thu tƒÉng', 'l·ª£i nhu·∫≠n tƒÉng'
    ]

    negative_words = [
        'gi·∫£m', 'l·ªó', 'ti√™u c·ª±c', 'x·∫•u', 's·ª•t gi·∫£m', 'r·ªßi ro',
        'kh√≥ khƒÉn', 't·ª•t', 'b·∫•t l·ª£i', 'lo ng·∫°i', 'suy y·∫øu',
        'ƒë√°y', 'ƒë·ªï v·ª°', 'bi quan', 'ƒë√† gi·∫£m', 'c·∫£nh b√°o',
        'n·ª£ x·∫•u', 'thua l·ªó', 'ph√° s·∫£n', 'kh·ªßng ho·∫£ng', 's·ª•p ƒë·ªï',
        'b√°n th√°o', 'lao d·ªëc', 'th√¢m h·ª•t', 'doanh thu gi·∫£m'
    ]

    text_lower = text.lower()
    pos_count = sum(1 for word in positive_words if word in text_lower)
    neg_count = sum(1 for word in negative_words if word in text_lower)

    total = pos_count + neg_count
    if total == 0:
        return 0.0

    score = (pos_count - neg_count) / total
    return round(score, 4)


def get_sentiment_for_stock(symbol: str, use_vietnamese: bool = True) -> pd.DataFrame:
    """
    Crawl tin t·ª©c v√† t√≠nh sentiment score cho m√£ c·ªï phi·∫øu.

    Args:
        symbol: M√£ c·ªï phi·∫øu
        use_vietnamese: True = d√πng t·ª´ ƒëi·ªÉn ti·∫øng Vi·ªát, False = d√πng TextBlob

    Returns:
        DataFrame (title, date, sentiment_score)
    """
    print(f"\nüîç Crawl & ph√¢n t√≠ch sentiment cho {symbol}...")

    # Crawl tin t·ª©c
    articles = crawl_cafef_news(symbol)
    articles.extend(crawl_vnexpress_news(symbol))

    if not articles:
        print(f"  ‚ö†Ô∏è Kh√¥ng t√¨m ƒë∆∞·ª£c tin t·ª©c cho {symbol}")
        return pd.DataFrame(columns=['title', 'date', 'sentiment_score'])

    # Ph√¢n t√≠ch sentiment
    results = []
    for article in articles:
        text = article.get('title', '') + ' ' + article.get('description', '')

        if use_vietnamese:
            score = analyze_sentiment_vietnamese(text)
        else:
            score = analyze_sentiment_textblob(text)

        results.append({
            'title': article.get('title', ''),
            'date': article.get('date', ''),
            'sentiment_score': score
        })

    df = pd.DataFrame(results)

    # Th·ªëng k√™
    avg_score = df['sentiment_score'].mean()
    sentiment_label = "T√≠ch c·ª±c üìà" if avg_score > 0 else "Ti√™u c·ª±c üìâ" if avg_score < 0 else "Trung l·∫≠p ‚û°Ô∏è"

    print(f"  üìä T·ªïng: {len(df)} tin t·ª©c")
    print(f"  üìä Sentiment trung b√¨nh: {avg_score:.4f} ({sentiment_label})")

    return df


def save_sentiment_data(df: pd.DataFrame, symbol: str) -> str:
    """L∆∞u sentiment data ra CSV."""
    os.makedirs(DATA_DIR, exist_ok=True)
    filepath = os.path.join(DATA_DIR, f"{symbol}_sentiment.csv")
    df.to_csv(filepath, index=False, encoding='utf-8-sig')
    print(f"  üíæ ƒê√£ l∆∞u sentiment: {filepath}")
    return filepath


if __name__ == "__main__":
    # Demo: crawl sentiment cho VNM
    df = get_sentiment_for_stock("VNM")
    if not df.empty:
        save_sentiment_data(df, "VNM")
        print(df.head(10))
